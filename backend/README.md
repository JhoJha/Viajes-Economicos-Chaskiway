# ğŸ› ï¸ Carpeta `backend`

Contiene todos los mÃ³dulos de backend responsables de la extracciÃ³n, procesamiento e integraciÃ³n de datos.

## Subcarpetas

- `scraping/`: Scripts para extraer datos de las fuentes (RedBus, clima, imÃ¡genes).
  - Cada subcarpeta (`redbus`, `clima`, `imagenes`) tiene su propio scraper y configuraciÃ³n.
- `database/`: Scripts para crear el esquema de la base de datos (`schema.py`), cargar los datos integrados (`loader.py`) y otras utilidades de integraciÃ³n.

## LÃ³gica principal

- Los scrapers descargan los datos crudos y los guardan en `data/raw/`.
- El pipeline de integraciÃ³n (`main.py` en la raÃ­z del proyecto) utiliza los mÃ³dulos de `database/` para combinar y cargar los datos en la base de datos final.
- El backend estÃ¡ diseÃ±ado para ser modular y fÃ¡cil de mantener.

## Â¿CÃ³mo usarlo?

1. Ejecuta los scrapers en `scraping/` para obtener los datos crudos.
2. Ejecuta `main.py` para integrar y cargar los datos en la base de datos. 