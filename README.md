# üöå Chaskiway: Recomendador de Viajes Econ√≥micos por el Per√∫

**Chaskiway** es una aplicaci√≥n web desarrollada como proyecto para el curso de Lenguaje de Programaci√≥n 2. Su objetivo es ayudar a los usuarios a planificar viajes interprovinciales desde Lima de manera inteligente, recomendando las mejores opciones en funci√≥n de su presupuesto, preferencias de clima y calidad del servicio.

---

## üéØ Objetivo del Proyecto

Desarrollar una aplicaci√≥n web interactiva que integre y analice datos de m√∫ltiples fuentes para ofrecer recomendaciones de viaje personalizadas. El proyecto demuestra la aplicaci√≥n de t√©cnicas de web scraping, procesamiento de datos, creaci√≥n de bases de datos y visualizaci√≥n de informaci√≥n en una soluci√≥n de software completa y funcional.

### Objetivos Espec√≠ficos
- **Recolectar** informaci√≥n de pasajes, clima e im√°genes de tres fuentes distintas.
- **Integrar** los datos recolectados en una √∫nica base de datos centralizada.
- **Desarrollar** un algoritmo de recomendaci√≥n simple basado en un sistema de puntuaci√≥n.
- **Crear** un dashboard visual e interactivo con Streamlit para presentar los resultados al usuario.

---

## üß± Arquitectura y Flujo de Datos

El proyecto sigue un pipeline de datos claro y modular para garantizar la calidad y consistencia de la informaci√≥n.

```
[Fuente 1: RedBus] --(Scraper)--> [Datos Crudos: JSONs] --+
                                                          |
[Fuente 2: API Clima] --(Scraper)--> [Datos Crudos: CSV] ----> [main.py: Combinaci√≥n] --> [Base de Datos SQLite] --> [Frontend: Streamlit]
                                                          |
[Fuente 3: API Im√°genes] --(Scraper)--> [Datos Crudos: JSON] --+
```

1.  **Extracci√≥n (Scraping):** Tres scripts independientes se encargan de recolectar los datos de sus respectivas fuentes y guardarlos en la carpeta `data/raw/`.
2.  **Combinaci√≥n y Carga (ETL):** El script `main.py` act√∫a como orquestador. Lee los tres conjuntos de datos crudos, los limpia y los combina usando la ciudad de destino como clave com√∫n. El resultado se carga en una base de datos SQLite en `data/processed/`.
3.  **Presentaci√≥n (Frontend):** La aplicaci√≥n `frontend/app.py` se conecta √∫nicamente a la base de datos final para leer los datos ya procesados y presentarlos al usuario.

---

## üìö Fuentes de Informaci√≥n

1.  **Portal RedBus:** Se utiliza para extraer informaci√≥n detallada de viajes (precios, horarios, empresas, ratings, asientos disponibles) mediante t√©cnicas de web scraping.
2.  **API Meteorol√≥gica (Visual Crossing):** Provee datos hist√≥ricos y de pron√≥stico del clima para cada ciudad de destino, permitiendo filtrar por preferencias clim√°ticas.
3.  **API de Im√°genes (Pixabay):** Suministra im√°genes representativas de alta calidad para cada destino, enriqueciendo la experiencia visual de la aplicaci√≥n.

---

## üöÄ Tecnolog√≠as Utilizadas

- **Lenguaje:** Python 3.11
- **Web Scraping:** `requests`, `beautifulsoup4` (si aplica)
- **Procesamiento de Datos:** `pandas`
- **Base de Datos:** `sqlite3`
- **Frontend:** `streamlit`
- **Control de Versiones:** Git y GitHub

---

## üë®‚Äçüíª Equipo de Desarrollo

| Integrante | Usuario de GitHub | Rol en el Proyecto |
| :--- | :--- | :--- |
| **Jhon Jhayro Villegas Verde** | `JhoJha` | Backend, Scraper de RedBus y Base de Datos |
| **Jonnathan Jes√∫s Pedraza Laboriano** | `[jonnathan2023]` | Backend, Scraper de Im√°genes y Frontend |
| **David Ojeda Valdiviezo** | `20210842` | Backend, Scraper de Clima y Dashboard |

---

## Git Workflow: Estrategia de Ramas

Para asegurar una colaboraci√≥n ordenada y eficiente, el proyecto utiliza un flujo de trabajo basado en ramas de funcionalidad (`feature branches`):

-   üå≥ **`main`**: Rama principal. Contiene √∫nicamente las versiones estables y funcionales del proyecto. Solo se fusiona desde `dev` cuando una versi√≥n ha sido probada y aprobada por el equipo.
-   üõ†Ô∏è **`dev`**: Rama de desarrollo e integraci√≥n. Es la rama donde se unen todos los avances. Antes de fusionar a `main`, todo debe funcionar correctamente en `dev`.
-   üöå **`scraper-redbus`**: Rama dedicada exclusivamente al desarrollo del scraper de RedBus y la l√≥gica de su base de datos. (Responsable: Jhon Villegas).
-   üå¶Ô∏è **`scraper-clima`**: Rama para el desarrollo del conector a la API de clima. (Responsable: David Ojeda Valdiviezo).
-   üñºÔ∏è **`scraper-imagenes`**: Rama para el desarrollo del conector a la API de im√°genes. (Responsable: Jonnathan Pedraza).
-   üñ•Ô∏è **`dashboard`**: Rama dedicada al desarrollo de la interfaz de usuario y las visualizaciones en Streamlit.

El flujo de trabajo es: cada integrante trabaja en su rama asignada, y una vez que su funcionalidad est√° completa, crea un **Pull Request** hacia la rama `dev` para su revisi√≥n e integraci√≥n.

---

## üìå Estado Actual del Proyecto

üöß **En Desarrollo.**

- [X] Creaci√≥n de la estructura base del proyecto y repositorio.
- [X] Definici√≥n de la estrategia de ramas y flujo de trabajo en Git.
- [ ] Desarrollo de los scrapers individuales.
- [ ] Dise√±o del esquema de la base de datos.
- [ ] Implementaci√≥n del pipeline de integraci√≥n de datos.
- [ ] Desarrollo de la interfaz de usuario en Streamlit.

---

## üìû Contacto

Para m√°s informaci√≥n, contactar a: `20231515@lamolina.edu.pe`

---

### üîé Sobre la extracci√≥n de datos de RedBus

En vez de realizar scraping tradicional de HTML (‚Äúscraping duro‚Äù), el equipo identific√≥ y utiliz√≥ la **API interna de RedBus**. Esto se logr√≥ mediante inspecci√≥n de la red en el navegador, donde se detect√≥ la solicitud POST que la web realiza para obtener los datos de viajes. A partir de esa solicitud, se construy√≥ el extractor que descarga los datos en formato JSON de manera estructurada y eficiente.

**Ventajas:**
- Mayor velocidad y menor riesgo de errores por cambios en el HTML.
- Datos m√°s limpios y estructurados.
- Menor carga para el servidor web.

**Nota:**
El uso de APIs internas requiere an√°lisis de tr√°fico de red y comprensi√≥n de c√≥mo la web interact√∫a con su backend, lo que demuestra habilidades avanzadas de scraping e ingenier√≠a inversa.

---

> **Recomendaci√≥n:** Para mayor claridad y mantenibilidad, cada carpeta principal del proyecto (por ejemplo, `backend/scraping/redbus`, `backend/scraping/clima`, `backend/database`, etc.) deber√≠a incluir un archivo `README.md` explicando brevemente su funci√≥n y c√≥mo usar los scripts que contiene.