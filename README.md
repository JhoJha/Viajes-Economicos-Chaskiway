# ğŸšŒ Chaskiway: Recomendador de Viajes EconÃ³micos por el PerÃº

**Chaskiway** es una aplicaciÃ³n web desarrollada como proyecto para el curso de Lenguaje de ProgramaciÃ³n 2. Su objetivo es ayudar a los usuarios a planificar viajes interprovinciales desde Lima de manera inteligente, recomendando las mejores opciones en funciÃ³n de su presupuesto, preferencias de clima y calidad del servicio.

---

## ğŸ¯ Objetivo del Proyecto

Desarrollar una aplicaciÃ³n web interactiva que integre y analice datos de mÃºltiples fuentes para ofrecer recomendaciones de viaje personalizadas. El proyecto demuestra la aplicaciÃ³n de tÃ©cnicas de web scraping, procesamiento de datos, creaciÃ³n de bases de datos y visualizaciÃ³n de informaciÃ³n en una soluciÃ³n de software completa y funcional.

### Objetivos EspecÃ­ficos
- **Recolectar** informaciÃ³n de pasajes, clima e imÃ¡genes de tres fuentes distintas.
- **Integrar** los datos recolectados en una Ãºnica base de datos centralizada.
- **Desarrollar** un algoritmo de recomendaciÃ³n simple basado en un sistema de puntuaciÃ³n.
- **Crear** un dashboard visual e interactivo con Streamlit para presentar los resultados al usuario.

---

## ğŸ§± Arquitectura y Flujo de Datos

El proyecto sigue un pipeline de datos claro y modular para garantizar la calidad y consistencia de la informaciÃ³n.

```
[Fuente 1: RedBus] --(Scraper)--> [Datos Crudos: JSONs] --+
                                                          |
[Fuente 2: API Clima] --(Scraper)--> [Datos Crudos: CSV] ----> [main.py: CombinaciÃ³n] --> [Base de Datos SQLite] --> [Frontend: Streamlit]
                                                          |
[Fuente 3: API ImÃ¡genes] --(Scraper)--> [Datos Crudos: JSON] --+
```

1.  **ExtracciÃ³n (Scraping):** Tres scripts independientes se encargan de recolectar los datos de sus respectivas fuentes y guardarlos en la carpeta `data/raw/`.
2.  **CombinaciÃ³n y Carga (ETL):** El script `main.py` actÃºa como orquestador. Lee los tres conjuntos de datos crudos, los limpia y los combina usando la ciudad de destino como clave comÃºn. El resultado se carga en una base de datos SQLite en `data/processed/`.
3.  **PresentaciÃ³n (Frontend):** La aplicaciÃ³n `frontend/app.py` se conecta Ãºnicamente a la base de datos final para leer los datos ya procesados y presentarlos al usuario.

---

## ğŸ“š Fuentes de InformaciÃ³n

1.  **Portal RedBus:** Se utiliza para extraer informaciÃ³n detallada de viajes (precios, horarios, empresas, ratings, asientos disponibles) mediante tÃ©cnicas de web scraping.
2.  **API MeteorolÃ³gica (Visual Crossing):** Provee datos histÃ³ricos y de pronÃ³stico del clima para cada ciudad de destino, permitiendo filtrar por preferencias climÃ¡ticas.
3.  **API de ImÃ¡genes (Pixabay):** Suministra imÃ¡genes representativas de alta calidad para cada destino, enriqueciendo la experiencia visual de la aplicaciÃ³n.

---

## ğŸš€ TecnologÃ­as Utilizadas

- **Lenguaje:** Python 3.11
- **Web Scraping:** `requests`, `beautifulsoup4` (si aplica)
- **Procesamiento de Datos:** `pandas`
- **Base de Datos:** `sqlite3`
- **Frontend:** `streamlit`
- **Control de Versiones:** Git y GitHub

---

## ğŸ‘¨â€ğŸ’» Equipo de Desarrollo y Ramas de Trabajo

El proyecto se desarrolla de forma colaborativa siguiendo el flujo de trabajo de GitFlow, con ramas especÃ­ficas para cada funcionalidad.

| Integrante | Usuario de GitHub | Rol en el Proyecto | Rama de Trabajo |
| :--- | :--- | :--- | :--- |
| **Jhon Jhayro Villegas Verde** | `JhoJha` | Backend, Scraper de RedBus y Base de Datos | `scraper-redbus` |
| **[Nombre CompaÃ±ero 1]** | `[UsuarioGitHub1]` | Backend, Scraper de Clima y Dashboard | `scraper-clima` / `dashboard` |
| **[Nombre CompaÃ±ero 2]** | `[UsuarioGitHub2]` | Backend, Scraper de ImÃ¡genes y Frontend | `scraper-imagenes` / `frontend` |

La rama `dev` se utiliza como entorno de integraciÃ³n antes de pasar las funcionalidades estables a la rama `main`.

---

## ğŸ“Œ Estado Actual del Proyecto

ğŸš§ **En Desarrollo.**

- [X] CreaciÃ³n de la estructura base del proyecto y repositorio.
- [ ] Desarrollo de los scrapers individuales.
- [ ] DiseÃ±o del esquema de la base de datos.
- [ ] ImplementaciÃ³n del pipeline de integraciÃ³n de datos.
- [ ] Desarrollo de la interfaz de usuario en Streamlit.

---

## ğŸ“ Contacto

Para mÃ¡s informaciÃ³n, contactar a: `20231515@lamolina.edu.pe`